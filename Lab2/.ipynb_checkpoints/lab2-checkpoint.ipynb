{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0.25363714 0.27225128 0.18778084]\n",
      "   [0.23852514 0.25642765 0.17319235]\n",
      "   [0.1812127  0.2544927  0.15840526]\n",
      "   ...\n",
      "   [0.13140519 0.13352211 0.08820476]\n",
      "   [0.16942532 0.06585877 0.01275396]\n",
      "   [0.12769338 0.20358719 0.1209075 ]]\n",
      "\n",
      "  [[0.306194   0.3279472  0.24048787]\n",
      "   [0.24980102 0.26248762 0.18839908]\n",
      "   [0.227339   0.21802898 0.20536718]\n",
      "   ...\n",
      "   [0.04027627 0.22851183 0.09719929]\n",
      "   [0.20763001 0.09454001 0.12148115]\n",
      "   [0.11754468 0.18129045 0.05649931]]\n",
      "\n",
      "  [[0.24751894 0.2852369  0.21910556]\n",
      "   [0.20745876 0.22057149 0.1499666 ]\n",
      "   [0.281523   0.2186363  0.20300779]\n",
      "   ...\n",
      "   [0.17136073 0.25682712 0.07194652]\n",
      "   [0.10839508 0.16333279 0.09038769]\n",
      "   [0.12609844 0.2556428  0.0639099 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.3276516  0.368524   0.30485547]\n",
      "   [0.24425758 0.30018517 0.23633379]\n",
      "   [0.28335565 0.33853245 0.2796687 ]\n",
      "   ...\n",
      "   [0.2665866  0.26259083 0.24436218]\n",
      "   [0.3386051  0.2706881  0.16607471]\n",
      "   [0.2766862  0.305955   0.24244756]]\n",
      "\n",
      "  [[0.33961213 0.35710084 0.3289693 ]\n",
      "   [0.26919082 0.3255184  0.25607693]\n",
      "   [0.26916066 0.33000642 0.24720518]\n",
      "   ...\n",
      "   [0.2499358  0.22643122 0.15393202]\n",
      "   [0.39193884 0.19846067 0.16985917]\n",
      "   [0.2935573  0.27230757 0.26682615]]\n",
      "\n",
      "  [[0.30921346 0.35490572 0.32240495]\n",
      "   [0.29279816 0.3386377  0.23162477]\n",
      "   [0.2926785  0.35556856 0.2406153 ]\n",
      "   ...\n",
      "   [0.2825383  0.32389984 0.21068889]\n",
      "   [0.36701977 0.32703698 0.20210361]\n",
      "   [0.26576546 0.29380724 0.21834919]]]], shape=(1, 384, 384, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#Directly Load Model\n",
    "from pathlib import PurePath\n",
    "import tensorflow as tf\n",
    "ModelBlock5 = tf.keras.models.load_model(str(PurePath('Model','Block5_Model')), compile = False)\n",
    "\n",
    "Image_sample = tf.zeros([384,384,3])\n",
    "#Make it 4D Tensor\n",
    "Image_sample_input = tf.expand_dims(Image_sample, 0)\n",
    "Out_image = ModelBlock5(Image_sample_input)\n",
    "print(Out_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "from pathlib import PurePath\n",
    "import sys\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class VGG19AE(tf.keras.Model):\n",
    "    def __init__(self, files_path):\n",
    "        super(VGG19AE, self).__init__()\n",
    "        #Load Model\n",
    "        ModelBlock5 = tf.keras.models.load_model(str(PurePath(files_path, 'Block5_Model')), compile = False)\n",
    "        #Get Each SubModel\n",
    "        self.E5 = ModelBlock5.layers[0]\n",
    "        self.D5 = ModelBlock5.layers[1]\n",
    "        self.O5 = ModelBlock5.layers[2]\n",
    "        ModelBlock4 = tf.keras.models.load_model(str(PurePath(files_path, 'Block4_Model')), compile = False)\n",
    "        self.E4 = ModelBlock4.layers[0]\n",
    "        self.D4 = ModelBlock4.layers[1]\n",
    "        self.O4 = ModelBlock4.layers[2]\n",
    "        ModelBlock3 = tf.keras.models.load_model(str(PurePath(files_path, 'Block3_Model')), compile = False)\n",
    "        self.E3 = ModelBlock3.layers[0]\n",
    "        self.D3 = ModelBlock3.layers[1]\n",
    "        self.O3 = ModelBlock3.layers[2]\n",
    "        ModelBlock2 = tf.keras.models.load_model(str(PurePath(files_path, 'Block2_Model')), compile = False)\n",
    "        self.E2 = ModelBlock2.layers[0]\n",
    "        self.D2 = ModelBlock2.layers[1]\n",
    "        self.O2 = ModelBlock2.layers[2]\n",
    "        ModelBlock1 = tf.keras.models.load_model(str(PurePath(files_path, 'Block1_Model')), compile = False)\n",
    "        self.E1 = ModelBlock1.layers[0]\n",
    "        self.O1 = ModelBlock1.layers[1]\n",
    "\n",
    "    def call(self, Image, training  = False):\n",
    "        # Input should be 4D Tensor\n",
    "        xo, I2 = Image\n",
    "        x = self.E2(xo)\n",
    "        style = self.E2(I2)\n",
    "        #Add WCT Here\n",
    "        #x = wct(x,style)\n",
    "        # x = self.wct(self,x,style)\n",
    "        x = self.D2(x)\n",
    "        x = self.O2(x)\n",
    "        # Block 1 Donot have decoder because it don't contain Pooling and there are only one Conv layer.\n",
    "        x = self.E1(x)\n",
    "        x = self.O1(x)\n",
    "        return tf.clip_by_value(tf.squeeze(x), 0, 1)\n",
    "AE = VGG19AE('Model')\n",
    "\n",
    "\n",
    "img_c = image.load_img(\"image/elephant.png\")\n",
    "img_c = image.img_to_array(img_c)\n",
    "img_c_shape = img_c.shape\n",
    "\n",
    "\n",
    "img_s = image.load_img(\"image/star.jpg\")\n",
    "img_s = image.img_to_array(img_s)\n",
    "img_s_shape = img_s.shape\n",
    "\n",
    "\n",
    "\n",
    "#Make it 4D Tensor\n",
    "Image_sample_input = (tf.expand_dims(img_c, 0), tf.expand_dims(img_s, 0))\n",
    "Out_image = AE(Image_sample_input)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(Out_image)\n",
    "plt.show()\n",
    "\n",
    "#print(Out_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wct(content, style, alpha=0.6, eps=1e-5):\n",
    "    '''\n",
    "    https://github.com/eridgd/WCT-TF/blob/master/ops.py\n",
    "       Perform Whiten-Color Transform on feature maps using numpy\n",
    "       See p.4 of the Universal Style Transfer paper for equations:\n",
    "       https://arxiv.org/pdf/1705.08086.pdf\n",
    "    '''\n",
    "    # 1xHxWxC -> CxHxW\n",
    "    content_t = np.transpose(np.squeeze(content), (2, 0, 1))\n",
    "    style_t = np.transpose(np.squeeze(style), (2, 0, 1))\n",
    "\n",
    "    # CxHxW -> CxH*W\n",
    "    content_flat = content_t.reshape(-1, content_t.shape[1]*content_t.shape[2])\n",
    "    style_flat = style_t.reshape(-1, style_t.shape[1]*style_t.shape[2])\n",
    "\n",
    "    # Whitening transform\n",
    "    mc = content_flat.mean(axis=1, keepdims=True)\n",
    "    fc = content_flat - mc\n",
    "    fcfc = np.dot(fc, fc.T) / (content_t.shape[1]*content_t.shape[2] - 1)\n",
    "    Ec, wc, _ = np.linalg.svd(fcfc)\n",
    "    k_c = (wc > 1e-5).sum()\n",
    "    Dc = np.diag((wc[:k_c]+eps)**-0.5)\n",
    "    fc_hat = Ec[:,:k_c].dot(Dc).dot(Ec[:,:k_c].T).dot(fc)\n",
    "\n",
    "    # Coloring transform\n",
    "    ms = style_flat.mean(axis=1, keepdims=True)\n",
    "    fs = style_flat - ms\n",
    "    fsfs = np.dot(fs, fs.T) / (style_t.shape[1]*style_t.shape[2] - 1)\n",
    "    Es, ws, _ = np.linalg.svd(fsfs)\n",
    "    k_s = (ws > 1e-5).sum()\n",
    "    Ds = np.sqrt(np.diag(ws[:k_s]+eps))\n",
    "    fcs_hat = Es[:,:k_s].dot(Ds).dot(Es[:,:k_s].T).dot(fc_hat)\n",
    "    fcs_hat = fcs_hat + ms\n",
    "\n",
    "    # Blend transform features with original features\n",
    "    blended = alpha*fcs_hat + (1 - alpha)*(fc)\n",
    "\n",
    "    # CxH*W -> CxHxW\n",
    "    blended = blended.reshape(content_t.shape)\n",
    "    # CxHxW -> 1xHxWxC\n",
    "    blended = np.expand_dims(np.transpose(blended, (1,2,0)), 0)\n",
    "\n",
    "    return np.float32(blended)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
